# Logestic-Regressor-With-All-Three-Batch-Stochastic-and-Mini-Batch-Gradient-Descent
I have implemented Logestic Regression for Binary classification from scratch with all three types of gradient descent.<br/>

===> main.ipynb is the notebook file that contains the implementation of the Logestic Regressor<br/>
===> nba_logreg.csv is the dataset on which the Regressor have been implemented, trained and then validated.<br/>

The code is quite generic, and can easily be modefied to fit on other binary Classifications datasets, tho the 'Data Cleaning' section of the notebook
would have to be modefied according to the dataset. <br/>

#NOTE: This is for binary class problems only.<br/>

Libraries used:<br/>
==> Sci-kit Learn: For train_test_splits, confusion_matrix, classification_report and accuracy report<br/>
==> Numpy: Since numpy calculations are faster, so all calculations are done with numpy<br/>
==> Pandas: To read the csv into a dataframe type<br/>
==> Matplotlib and Seaborn: For plotting graphs and heatmaps<br/>
==> Random: To intialize weights randomly at beginning 
